<!DOCTYPE html>
<html lang="en-US">

<head>
	<meta name="generator" content="Hugo 0.16" />
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
<title> The Exclusive Top Floor algorithm </title>


<link rel="stylesheet" href="https://sbarral.github.io/etf/css/boring.css">
<link rel="stylesheet" href="https://sbarral.github.io/etf/css/highlight.min.css">
<link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700%7cRoboto+Slab:700%7cJosefin+Sans:400%7cSource+Code+Pro' rel='stylesheet' type='text/css'>


<link href="https://sbarral.github.io/etf/index.xml" rel="alternate" type="application/rss+xml" title="The Exclusive Top Floor algorithm" />

</head>

<body>
  <header  style="background-image: url(https://sbarral.github.io/etf/img/etf_by_rozalia.jpg)"  > 
 
 <div id="links">
 <a target="_blank" href=https://github.com/sbarral/etf-cpp>
   <div class="icon-github-circled"></div>
 </a>
 </div>
 
 <div id="legend">(Artwork: Rozalia F.B. — Mixed media)</div>
</header>

  <div class="container">
    <section>
      <h1>The Exclusive Top Floor algorithm</h1>
      <h2>Super-fast univariate distributions sampling</h2>
    </section>
    <article class="content">
      

<p><em>TL;DR: the Exclusive Top Floor algorithm is a blazingly fast algorithm to sample
arbitrary continuous probability distributions solely from their probability
density function (PDF) and using only marginally more than one random number
per sample.</em></p>

<p><em>A free reference implementation is provided as a
<a href="https://github.com/sbarral/etf-cpp">C++ library</a>.</em></p>

<h2 id="why-should-i-use-it">Why should I use it?</h2>

<p>The ETF algorithm has a number of advantages over other methods, including:</p>

<ul>
<li><p><strong>speed</strong>: it is comparable in speed to the
<a href="https://en.wikipedia.org/wiki/Ziggurat_algorithm">ziggurat</a> algorithm
<a href="#references">[1]</a> and thus typically many times faster than
<a href="https://en.wikipedia.org/wiki/Inverse_transform_sampling">inversion sampling</a>,</p></li>

<li><p><strong>quality</strong>: the entropy of the generated distribution is
<a href="#quality">noticeably higher</a>
than that of the ziggurat and quite close to that of inversion sampling; in
fact, ETF will even outperform inversion sampling in terms of quality in the
common cases where the RNG produces more bits than the precision of the
floating-point type used (e.g. 32-bit random numbers paired with
single-precision floats or 64-bit random numbers paired with double-precision
floats).</p></li>

<li><p><strong>convenience</strong>: there is no need to know the cumulative distribution
function (CDF) and even less its inverse; the lookup tables can be
automatically generated with a multivariate newton method provided within the
library, requiring only the PDF and its derivative (though even the PDF
derivative could be omitted with the use of e.g. a multivariate secant method),</p></li>

<li><p><strong>versatility</strong>: unlike the ziggurat, it can be applied to a wide range of
distributions including non-monotonic or asymmetric distributions; also,
the <em>x</em> position beyond which the tail is sampled with a fallback algorithm
can be arbitrarily adjusted without consideration for the statistical weight
of the tail,</p></li>

<li><p><strong>statistically independent samples</strong>: unlike
<a href="https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo">Markow chain Monte Carlo methods</a>
such as the Metropolis-Hastings and slice sampling algorithms, the ETF method
does not introduce correlations between samples.</p></li>

<li><p><strong>thread-safety</strong>: unlike some distribution-specific methods such as the
Box-Muller transform or the polar method, the ETF algorithm is inherently
stateless and numerical distributions can thus be safely and efficiently shared
between threads (though thread-local RNGs are still required, of course).</p></li>
</ul>

<p>So what is the catch?</p>

<p>Although it is unlikely to matter in practice, there is a small space vs
speed trade-off: the reference implementation needs 3 floating-point tables
and 1 unsigned integer table with typically 128 or 256 elements each.</p>

<p>Also, if the parameters of a distributions are not statically known and the
distribution cannot be generated by cheap transformation of a fixed-parameters
distribution, then the tables must be generated dynamically.
This one-time cost is usually easily amortized, however, since table generation
has only <em>Ο(n)</em> complexity and, in the case of Newton&rsquo;s method, a quadratic
convergence rate with respect to precision.</p>

<p><strong>The bottom line</strong>: unless only relatively few samples are to be generated,
the ETF method is likely to provide substantial benefits with very few
downsides.</p>

<h2 id="how-does-it-work">How does it work?</h2>

<h3 id="the-big-picture">The big picture</h3>

<p>The underlying idea of ETF is simple: a majorizing function for the PDF is
defined using adjacent vertically laid rectangles forming a &ldquo;city skyline&rdquo;, as
in an upper <a href="https://en.wikipedia.org/wiki/Riemann_sum">Riemann sums</a>. Instead
of splitting the <em>x</em> interval evenly, however, the partition is specifically
chosen so that all rectangles have equal areas. A minorizing function is in
turn defined with somewhat smaller rectangles laid out on the same
sub-intervals. The <em>top floors</em> giving their name to the method are the small
rectangular areas filling the space between minorizing and majorizing
rectangles.</p>

<p>Using the real estate analogy, inhabitants are initially distributed evenly
within the buildings by choosing a location at random within the area under the
majorizing function, i.e. anywhere between the ground and the skyline.  To do
so, a building is first chosen at random (equiprobably since upper rectangles
have the same area) and then a random location within that building.  If an
inhabitant end up on a lower floor, its location is accepted right away without
further computations.</p>

<p>If an inhabitant ends up on a top floor, however, he will need to prove that
he really belong there: the PDF is evaluated at the vertical of the location to
check whether it indeed lies under the PDF, failing which the inhabitant will
be expropriated.
Staying true to the analogy, the lower floors are therefore (computationally)
much cheaper than the topmost floors but fortunately are chosen much more
often, thus keeping the average cost at a low level.</p>

<p>Implemented naively, however, this method would still require two or even three
random numbers per sample: one to choose the building and two to choose the
vertical and horizontal coordinates within the building. The method wouldn&rsquo;t be
very new either <a href="#references">[2]</a>.</p>

<h3 id="the-gory-details">The gory details</h3>

<p>So here is how it <em>really</em> works.
Let us consider for the sake of simplicity a non-symmetric distribution with
compact support.
The algorithm starts by computing the majorizing and minorizing functions as
outlined above, typically using a partition with 128 or 256 sub-intervals.
Ignoring some additional optimizations made in the actual implementation,
random variates are generated as follows:</p>

<ol>
<li><p>a <em>W</em>-bit random integer is generated,</p></li>

<li><p>since all upper rectangles have equal area, a rectangle can be chosen at
random from a number extracted from <em>N</em> of the available <em>W</em> random bits
(say <em>N</em>=7 provided that there are 128 majorizing rectangles).</p></li>

<li><p>the remaining <em>W</em>-<em>N</em> independent bits are used to generate a real number
<em>y</em> between 0 and <em>H</em> where <em>H</em> is the height of the upper rectangle,</p></li>

<li><p>if <em>y</em> is larger than the height <em>h</em> of the lower rectangle then jump to 6
(wedge sampling, a.k.a. top floor test), otherwise proceed to 5 (acceptance),</p></li>

<li><p>[<strong>here is the trick</strong>]: we are left with <em>y</em>, a nice number uniformly
distributed in [0, <em>h</em>) with almost the same entropy as the untested
<em>y</em> —we have only lost <em>log₂(H/h)</em> bits— so why throw it to the garbage?
Instead, we apply an affine transformation to <em>y</em> from interval [0, <em>h</em>) to
interval [<em>xᵢ</em> and <em>xᵢ₊₁</em>) and return the number <em>x</em> thus produced.</p></li>

<li><p>[<strong>wedge sampling: this happens rarely</strong>] another random number
<em>x</em> within the current rectangle interval [<em>xᵢ</em> and <em>xᵢ₊₁</em> ) is generated
and <em>y</em> is compared at location <em>x</em> with the PDF envelope; if it is under the
PDF, return <em>x</em>, otherwise jump back to 1.</p></li>
</ol>

<p>A legitimate question at this point would be whether we have not irremediably
lost the <em>N</em> bits of precision used to generate the quantile index.</p>

<p>The answer is no: the generation of <em>x</em> does in fact require less precision
since, once the quantile has been determined, the range of possible <em>x</em> values
has already narrowed down the interval to [<em>xᵢ</em> and <em>xᵢ₊₁</em>), and since there
are 2<em>ᴺ</em> such intervals, one needs this much less precision.  In fact, if the
function to sample was the skyline majorizing function (i.e.  if the top floor
rejection step could be omitted), the method could be shown to be nothing else
that a (very) fast inversion sampling method without any loss of quality. The
overall quality loss is thus in theory relatively moderate and mostly imputable
to wedge rejection.</p>

<p>For tailed distributions, the algorithm admits an externally supplied
(fallback) tail distribution, just like the ziggurat algorithm; unlike the
ziggurat, however, it does not require the area of the tail to be equal to that
of the quantiles, thus relaxing the constrain on the <em>x</em> threshold beyond which
the fallback tail needs to be used. One may also use instead a majorizing PDF
for the fallback and apply rejection sampling in the tail.</p>

<p>To include a tail, step 4 above is modified to generate a real number <em>y</em>
between 0 and <em>H/(1-P(tail))</em> where <em>P(tail)</em> is the tail sampling probability
(i.e. the area of the tail divided by the total area of the majorizing function,
tail included).
If <em>y</em> is greater than <em>H</em>, then the tail is sampled.</p>

<p>Note that for the sake of efficiency, in the actual implementation <em>y</em> is an
integer and the various rejection thresholds are appropriately scaled so as
to only use integer arithmetic until <em>x</em> is to be computed.</p>

<h2 id="quality">Quality</h2>

<h3 id="quality-qualitatively">Quality, qualitatively</h3>

<p>The ETF algorithm is RNG-agnostic so potential quality issued related to the
RNG itself will not be investigated.
Also, since this is a rejection method, goodness of fit is assumed without
demonstration.</p>

<p>The main question becomes therefore: how much of the quality of the original
RNG does it actually retain? Or alternatively, how &ldquo;uniformly&rdquo; (in a loose
meaning) would the numbers be distributed if we used an ideal RNG?  From this
perspective, inversion sampling can be considered the reference method: baring
rounding issues and provided that the floating point mantissa has more digits
that the random number, one can always map a number generated by inversion
sampling back to the original random number using the CDF.</p>

<p>One way to crudely assess the quality is by sampling the distribution with a
high quality RNG, using many more samples than the range 2<em>ᵂ</em> of the RNG. If we
set for now our ambitions and <em>W</em> low, at say 16 bits, this is fairly easy to
do.</p>

<p>Without further ado, this is the result of the collection of 10⁹ normal
variates in 1001 bins, generated with the ziggurat and the ETF algorithms with
a 16-bit RNG (actually, a truncated MT19937).</p>

<p><img src="https://sbarral.github.io/etf/img/fig_16bit_comparison.png" alt="Normal variates from ziggurat and ETF" /></p>

<p>Yes, quite a difference.</p>

<p>It is important to note that these are the converged numerical PDFs:
<strong>all observable deviations from the theoretical PDF are inherent to the
algorithm used and are not related to the RNG</strong>: adding more samples or using
another (good) RNG would not change these plots in any visible manner. In fact,
a testimony to convergence can be directly found in the symmetry of the scatter
plots.</p>

<h3 id="quality-quantitatively">Quality, quantitatively</h3>

<p>But can quality be assessed in a more rigorous manner, especially at higher
precision <em>W</em>? To some extent, yes. A test that can point to poor distribution
of the variates at small scales without requiring too much computing power is
the Knuth collision test <a href="#references">[3]</a> where <em>n</em> balls are randomly thrown
into 2<em>ᵈ</em> urns.  Even with <em>n</em>&lt;2<em>ᵈ</em>, the number of collisions (i.e. balls that
get into an already filled urn) can be sufficient to form the basis for a
statistical test that assesses uniformity defects.</p>

<p>Following Doornik <a href="#references">[4]</a> we first generate normal variates and then
use the normal CDF to transform them back to a random number that <em>should</em> be
uniformly distributed in [0, 1). The Knuth collision test is then performed
for several values of <em>d</em>, keeping <em>n</em> smaller than 2<em>ᵈ</em> by a factor of 256.
The RNG is a MT19937 with <em>W</em>=32 bits and all floating point computations are
performed in double precision.  The ziggurat and ETF algorithm both use
128-entry tables.  The inversion sampling test used for comparison is actually
an ideal case: it is simulated by simply transforming the RNG integer directly
to a uniform number in [0, 1), which is akin to performing inversion sampling
and then transforming back the variate with the normal CDF using infinite
floating point precision.</p>

<p>So here are the results, with each test repeated 10 times for each value of <em>d</em>.
See the <a href="benchmark/collision.cpp">source</a> for details (note that
the Boost special functions library is required).</p>

<p><img src="https://sbarral.github.io/etf/img/fig_pvalue.png" alt="p-values with d between 26 and 33" /></p>

<p>Without going into details, if the variates are well distributed then the
10 p-values (scatter plot) should be uniformly distributed between 0 and 1 and
their mean value (solid line) should be close to 0.5. When all p-values go
low (the traditional threshold is 5%), this is a sure sign that the test has
failed.</p>

<p>So what does this plot say?</p>

<p>Expectedly all algorithms fail at <em>d</em>=33 since with <em>W</em>=32 bits of information
it is obviously impossible to evenly distribute balls into 2<em>³³</em> urns.
Simulated inversion sampling succeeds at <em>d</em>=32, but this was also expected
since this ideal inversion sampling test is de-facto a measure of the quality
of mt19937 (which is known to be good).</p>

<p>The quality of the ETF algorithm is very satisfactory with a loss of only 2
effective bits compared to ideal inversion sampling. The ziggurat algorithm is
clearly worse with a 5-bit effective loss compared to ideal inversion sampling,
thus confirming the hint given by our former visual comparison of distributions
generated with <em>W</em>=16.</p>

<h2 id="speed">Speed</h2>

<p>The ziggurat was of course used as the reference algorithm for speed since it
is widely considered the fastest method to generate normal variates of
reasonable quality.</p>

<p>In order to provide a fair comparison, the ziggurat algorithm used as baseline
was also made portable and generic over the RNG precision (parameter <em>W</em>). At
the same time, an apparently yet unreported bug in the original ziggurat
algorithm which seems to have propagated to many implementations was fixed (see
comments in the
<a href="https://github.com/sbarral/etf-cpp/benchmark/ziggurat_normal.hpp">source code</a>).
The original ziggurat algorithm is also benchmarked as a sanity check to make
sure that the speed of the corrected and precision-generic algorithm remains
comparable.</p>

<p>The
<a href="https://github.com/sbarral/etf-cpp/benchmark/etf_normal.hpp">ETF implementation</a>
reflects most of the feature of the corrected ziggurat implementation but is
additionally generic over the number of bits <em>N</em> of the table index whereas the
ziggurat was hardcoded with <em>N</em>=7.  In order to compare apples to apples, all
tests are performed with <em>N</em>=7 for both the ziggurat and the ETF algorithms.</p>

<p>The speed benchmark was run on a i5-3230M (yes I know — I accept donations).
It sums 10000 normal variates with either the 32-bit MT19937 (for <em>W</em>=32) or
with the 64-bit MT19937 (for <em>W</em>=64) of the C++ standard library as RNG.
Each of the reported timing is an average over 1000 tests.
Feel free to contribute your timings using the
<a href="https://github.com/sbarral/etf-cpp/benchmark/timing.cpp">benchmarking code</a>.</p>

<table>
<thead>
<tr>
<th><strong>W=32 bits</strong></th>
<th>gcc 4.9.2 (-O2)</th>
<th>clang 3.5.0 (-O2)</th>
</tr>
</thead>

<tbody>
<tr>
<td>Generic, portable, fixed ziggurat</td>
<td>71.9 µs</td>
<td>97.6 µs</td>
</tr>

<tr>
<td>Original ziggurat</td>
<td>66.6 µs   (-7%)</td>
<td>95.4 µs    (-2%)</td>
</tr>

<tr>
<td>Exclusive top floor (generic, portable)</td>
<td>72.7 µs   (+1%)</td>
<td>115.9 µs   (+19%)</td>
</tr>
</tbody>
</table>

<table>
<thead>
<tr>
<th><strong>W=64 bits</strong></th>
<th>gcc 4.9.2 (-O2)</th>
<th>clang 3.5.0 (-O2)</th>
</tr>
</thead>

<tbody>
<tr>
<td>Generic, portable, fixed ziggurat</td>
<td>80.4 µs</td>
<td>97.5 µs</td>
</tr>

<tr>
<td>Original ziggurat</td>
<td>70.5 µs  (-12%)</td>
<td>95.7 µs    (-2%)</td>
</tr>

<tr>
<td>Exclusive top floor (generic, portable)</td>
<td>81.4 µs   (+1%)</td>
<td>112.1 µs   (+15%)</td>
</tr>

<tr>
<td>libstdc++ (polar transform)</td>
<td>437.7 µs (+444%)</td>
<td>1440.7 µs (+1378%)</td>
</tr>
</tbody>
</table>

<p>Despite a few oddities like clang taking longer to compute with a 32-bit RNG
than with a 64 bit RNG and being in general worse at optimising the ETF, the
picture is very consistent: the difference between the baseline ziggurat and
the ETF is barely noticeable with a penalty limited to 1% on gcc and 19% on
clang, compared to an abysmal +444% and +1378% for the native libstdc++
implementation of the normal distribution which uses the polar transform.
Tests with -O3 give nearly identical figures for gcc and actually slightly
worse figures on clang for both ziggurat versions as well as for the ETF (by a
couple of %), the only gain being observed on the libstdc++ polar transform
which drops to &ldquo;only&rdquo; 931 µs.</p>

<h2 id="conclusion">Conclusion</h2>

<p>The ETF method generates samples which quality is substantially higher
than that achievable with the fastest algorithm to date—the ziggurat method—for
a roughly equivalent computational cost and a much greater versatility
regarding its applicability to arbitrary continuous univariate distributions.</p>

<h2 id="todo">Todo</h2>

<p>Hopefully a Rust implementation will follow soon.</p>

<p>Contributors for ports to other languages are very welcome.</p>

<h2 id="references">References</h2>

<p>[1] G. Marsaglia and W. W. Tsang,
<a href="https://www.jstatsoft.org/article/view/v005i08">&ldquo;The ziggurat method for generating random variables&rdquo;</a>,
<em>Journal of Statistical Software 5</em>, 1-7 (2000).</p>

<p>[2] R. Zhang and L. M. Leemis,
<a href="http://dx.doi.org/10.1002/nav.21474">&ldquo;Rectangles algorithm for generating normal variates&rdquo;</a>,
<em>Naval Research Logistics 59(1)</em>, 52-57 (2012).</p>

<p>[3] D. E. Knuth, &ldquo;The art of computer programming&rdquo;,
<em>Vol. 2</em>, 3rd ed., Addison-Wesley (1997).</p>

<p>[4] J. A. Doornik,
<a href="http://www.doornik.com/research/ziggurat.pdf">&ldquo;An improved ziggurat method to generate normal random samples&rdquo;</a>,
Technical note, University of Oxford (2005).</p>

    </article>
    <footer>
  <p>Serge BARRAL │ Last updated: 10 OCT 2016</p>
</footer>

  </div>
  <script src="https://sbarral.github.io/etf/js/highlight.min.js"></script>
  <script> hljs.initHighlightingOnLoad(); </script>
</body>

</html>
